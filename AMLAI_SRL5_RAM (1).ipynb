{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "\n",
    "# Advanced Machine Learning and Artificial Intelligence\n",
    "\n",
    "# Reinforcement Learning\n",
    "\n",
    "## Notebook 6: Learning Ms. Pac-Man with DQN\n",
    "\n",
    "## Yuri Balasanov, Mihail Tselishchev, &copy; iLykei 2018\n",
    "\n",
    "##### Main text: Hands-On Machine Learning with Scikit-Learn and TensorFlow, Aurelien Geron, &copy; Aurelien Geron 2017, O'Reilly Media, Inc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:22.281975Z",
     "start_time": "2019-03-18T20:11:19.082926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, InputLayer\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import gym\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning of MS. Pac-Man with Keras\n",
    "\n",
    "This notebook shows how to implement a deep neural network approach to train an agent to play Ms.Pac-Man Atari game.\n",
    "\n",
    "\n",
    "## Explore the game\n",
    "\n",
    "Use [Gym](https://gym.openai.com/) toolkit that provides both game environment and also a convenient renderer of the game.\n",
    "\n",
    "Create an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:23.530569Z",
     "start_time": "2019-03-18T20:11:23.153229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/targoon/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "env.action_space  # actions are integers from 0 to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to play the game using random strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:26.071214Z",
     "start_time": "2019-03-18T20:11:25.387650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 290.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = random.randrange(env.action_space.n)  # select random action\n",
    "    obs, reward, done, info = env.step(action)     # make action and get results\n",
    "    score += reward\n",
    "    #env.render()\n",
    "    #time.sleep(0.01)\n",
    "    \n",
    "env.close()\n",
    "print('Score =', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "In this environment, observation (i.e. current state) is the RAM of the Atari machine, namely a vector of 128 bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T01:06:59.755667Z",
     "start_time": "2019-03-05T01:06:59.740422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape = (128,)\n",
      "obs dtype = uint8\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print('obs shape =', obs.shape)\n",
    "print('obs dtype =', obs.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T01:49:57.317543Z",
     "start_time": "2019-02-12T01:49:57.307550Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 112 114 115  48   3  88  88  88  88  88   0  80  80  80  50  98   0\n",
      "   0   3   0   0   1   0   0   1   6   6 198   4  63   0  45   1   0 198\n",
      " 198   0   0   0   0  32  52   0   0 120   0 100 130   0   0 134   1 222\n",
      "   0   1   3   0   6  80 255 255   0 255 255  80 255 255  80 255 255  80\n",
      " 255 255  80 191 191  80 191 191  80 191 191  80 255 255  80 255 255  80\n",
      " 255 255  80 255 255   0 255 255  80 255 255  20 223  43 217 123 217 123\n",
      " 217 123 217 123 217 123 217 221   0  63   0   0   0   0   0   2  66 240\n",
      " 146 215]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deep neural network that takes byte vector as an input and produces Q-values for state-action pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN-model using Keras\n",
    "\n",
    "The following model is of the same general type applied to the cartPole problem.\n",
    "\n",
    "Use vanilla multi-layer dense network with relu activations which computes Q-values $Q(s,a)$ for all states $s$ and actions $a$ (with some discount factor $\\gamma$).\n",
    "This neural network denoted by $Q(s\\ |\\ \\theta)$ takes current state as an input and produces a vector of q-values for all 9 possible actions. Vector $\\theta$ corresponds to all trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:36.802493Z",
     "start_time": "2019-03-18T20:11:36.793757Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, dense_layers, dense_units):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for i in range(dense_layers):\n",
    "        model.add(Dense(units=dense_units, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a network using specific input shape and action space size. We call this network *online*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:39.012502Z",
     "start_time": "2019-03-18T20:11:38.849223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 364,297\n",
      "Trainable params: 364,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = obs.shape\n",
    "nb_actions = env.action_space.n  # 9\n",
    "dense_layers = 6\n",
    "dense_units = 256\n",
    "\n",
    "online_network = create_dqn_model(input_shape, nb_actions, dense_layers, dense_units)\n",
    "online_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T01:07:04.711293Z",
     "start_time": "2019-03-05T01:07:04.703755Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "#plot_model(online_network, to_file='online_DenseNetwork.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the architecture of the network saved as *online_DenseNetwork.png*. (To see the plot log into iLykei.com, then rerun this cell).\n",
    "\n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FReinforced%20Learning%2Fonline_DenseNetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is used to explore states and rewards of Markov decision process according to an $\\varepsilon$-greedy exploration strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:46.154834Z",
     "start_time": "2019-03-18T20:11:46.150036Z"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon, n_outputs):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_outputs)  # random action\n",
    "    else:\n",
    "        return np.argmax(q_values)          # q-optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online network stores explored information in a *replay memory*, a double-ended queue (deque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:47.410722Z",
     "start_time": "2019-03-18T20:11:47.401804Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_maxlen = 1000000\n",
    "replay_memory = deque([], maxlen=replay_memory_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, online network explores the game using $\\varepsilon$-greedy strategy and saves experienced transitions in replay memory. \n",
    "\n",
    "In order to produce Q-values for $\\varepsilon$-greedy strategy, following the proposal of the [original paper by Google DeepMind](https://www.nature.com/articles/nature14236), use another network, called *target network*, to calculate \"ground-truth\" target for the online network. *Target network*, has the same architecture as online network and is not going to be trained. Instead, weights from the online network are periodically copied to target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:49.620021Z",
     "start_time": "2019-03-18T20:11:49.434669Z"
    }
   },
   "outputs": [],
   "source": [
    "target_network = clone_model(online_network)\n",
    "target_network.set_weights(online_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target network uses past experience in the form of randomly selected records of the replay memory to predict targets for the online network: \n",
    "\n",
    "- Select a random minibatch from replay memory containing tuples $(\\text{state},\\text{action},\\text{reward},\\text{next_state})$\n",
    "\n",
    "- For every tuple $(\\text{state},\\text{action},\\text{reward},\\text{next_state})$ from minibatch Q-value function $Q(\\text{state},\\text{action}\\ |\\ \\theta_{\\text{online}})$ predicts value according to Bellman-type equation: \n",
    "\n",
    "$$y_\\text{target} = \\text{reward} + \\gamma \\cdot \\max_a Q(\\text{next_state}, a\\ |\\ \\theta_\\text{target})$$\n",
    "if the game continues and $$ y_\\text{target} = \\text{reward}$$ if the game has ended. \n",
    "\n",
    "Note that at this step predictions are made by the target network. This helps preventing situations when online network simultaneously predicts values and creates targets, which might potentially lead to instability of training process.\n",
    "\n",
    "- For each record in the minibatch targets need to be calculated for only one specific $\\text{action}$ output of online network. It is important to ignore all other outputs during optimization (calculating gradients). So, predictions for every record in the minibatch are calculated by online network first, then the values corresponding to the taction of interest are replaced with ones predicted by target network. \n",
    "\n",
    "Proposed approach is called **DQN**-approach. However, [it is known](https://arxiv.org/abs/1509.06461) to overestimate q-values under certain conditions. In the paper another approach was proposed, called **Double DQN**. Instead of taking action that maximizes q-value for target network, they pick an action that maximizes q-value for online network as an optimal one:\n",
    "\n",
    "$$y_\\text{target} = \\text{reward} + \\gamma \\cdot Q\\left(\\text{next_state}, \\arg\\max_a Q\\left(\\text{next_state},a\\ |\\ \\theta_\\text{online}\\right)\\ |\\ \\theta_\\text{target}\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DQN\n",
    "\n",
    "First, define hyperparameters (Do not forget to change them before moving to cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:51.742492Z",
     "start_time": "2019-03-18T20:11:51.733807Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'MsPacman_DQN_0214191034'  # used in naming files (weights, logs, etc)\n",
    "n_steps = 150000        # total number of training steps (= n_epochs)\n",
    "warmup = 5000          # start training after warmup iterations\n",
    "training_interval = 4  # period (in actions) between training steps\n",
    "save_steps = int(n_steps/10)  # period (in training steps) between storing weights to file\n",
    "copy_steps = 1000       # period (in training steps) between updating target_network weights\n",
    "gamma = 0.85            # discount rate\n",
    "skip_start = 90        # skip the start of every game (it's just freezing time before game starts)\n",
    "batch_size = 100       # size of minibatch that is taken randomly from replay memory every training step\n",
    "double_dqn = True     # whether to use Double-DQN approach or simple DQN (see above)\n",
    "# eps-greedy parameters: we slowly decrease epsilon from eps_max to eps_min in eps_decay_steps\n",
    "eps_max = 1.0\n",
    "eps_min = 0.05\n",
    "eps_decay_steps = int(n_steps/2)\n",
    "\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile online-network with Adam optimizer, mean squared error loss and `mean_q` metric, which measures the maximum of predicted q-values averaged over samples from minibatch (we expect it to increase during training process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:54.593033Z",
     "start_time": "2019-03-18T20:11:54.538600Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_q(y_true, y_pred):\n",
    "    return K.mean(K.max(y_pred, axis=-1))\n",
    "\n",
    "\n",
    "online_network.compile(optimizer=Adam(learning_rate), loss='mse', metrics=[mean_q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folder for logs and trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:11:55.899714Z",
     "start_time": "2019-03-18T20:11:55.886793Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(name):\n",
    "    os.makedirs(name)\n",
    "    \n",
    "weights_folder = os.path.join(name, 'weights')\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use standard callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(os.path.join(name, 'log.csv'), append=True, separator=';')\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(name, 'tensorboard'), write_graph=False, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next chunk of code explores the game, trains online network and periodically copies weights to target network as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counters:\n",
    "step = 0          # training step counter (= epoch counter)\n",
    "iteration = 0     # frames counter\n",
    "episodes = 0      # game episodes counter\n",
    "done = True       # indicator that env needs to be reset\n",
    "\n",
    "episode_scores = []  # collect total scores in this list and log it later\n",
    "\n",
    "while step < n_steps:\n",
    "    if done:  # game over, restart it\n",
    "        obs = env.reset()\n",
    "        score = 0  # reset score for current episode\n",
    "        for skip in range(skip_start):  # skip the start of each game (it's just freezing time before game starts)\n",
    "            obs, reward, done, info = env.step(0)\n",
    "            score += reward\n",
    "        state = obs\n",
    "        episodes += 1\n",
    "\n",
    "    # Online network evaluates what to do\n",
    "    iteration += 1\n",
    "    q_values = online_network.predict(np.array([state]))[0]  # calculate q-values using online network\n",
    "    # select epsilon (which linearly decreases over training steps):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    action = epsilon_greedy(q_values, epsilon, nb_actions)\n",
    "    # Play:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        episode_scores.append(score)\n",
    "    next_state = obs\n",
    "    # Let's memorize what just happened\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "\n",
    "    if iteration >= warmup and iteration % training_interval == 0:\n",
    "        #if iteration % 1000 == 0:\n",
    "         #   print(iteration)\n",
    "        # learning branch\n",
    "        step += 1\n",
    "        minibatch = random.sample(replay_memory, batch_size)\n",
    "        replay_state = np.array([x[0] for x in minibatch])\n",
    "        replay_action = np.array([x[1] for x in minibatch])\n",
    "        replay_rewards = np.array([x[2] for x in minibatch])\n",
    "        replay_next_state = np.array([x[3] for x in minibatch])\n",
    "        replay_done = np.array([x[4] for x in minibatch], dtype=int)\n",
    "\n",
    "        # calculate targets (see above for details)\n",
    "        if double_dqn == False:\n",
    "            # DQN\n",
    "            target_for_action = replay_rewards + (1-replay_done) * gamma * \\\n",
    "                                    np.amax(target_network.predict(replay_next_state), axis=1)\n",
    "        else:\n",
    "            # Double DQN\n",
    "            best_actions = np.argmax(online_network.predict(replay_next_state), axis=1)\n",
    "            target_for_action = replay_rewards + (1-replay_done) * gamma * \\\n",
    "                                    target_network.predict(replay_next_state)[np.arange(batch_size), best_actions]\n",
    "\n",
    "        target = online_network.predict(replay_state)  # targets coincide with predictions ...\n",
    "        target[np.arange(batch_size), replay_action] = target_for_action  #...except for targets with actions from replay\n",
    "        \n",
    "        # Train online network\n",
    "        online_network.fit(replay_state, target, epochs=step, verbose=0, initial_epoch=step-1,\n",
    "                           callbacks=[csv_logger, tensorboard])\n",
    "\n",
    "        # Periodically copy online network weights to target network\n",
    "        if step % copy_steps == 0:\n",
    "            target_network.set_weights(online_network.get_weights())\n",
    "        # And save weights\n",
    "        if step % save_steps == 0:\n",
    "            online_network.save_weights(os.path.join(weights_folder, 'weights_{}.h5f'.format(step)))\n",
    "            gc.collect()  # also clean the garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save last weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_network.save_weights(os.path.join(weights_folder, 'weights_last_1.h5f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_network.save('saved_dqn_model_ms_pacman.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to check TensorBoard for fancy statistics on loss and metrics using\n",
    "\n",
    "*tensorboard --logdir=tensorboard*\n",
    "\n",
    "and visit http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "\n",
    "Finally, create a function to evalutate the trained network. \n",
    "Note that we still using $\\varepsilon$-greedy strategy here to prevent an agent from getting stuck. \n",
    "`test_dqn` returns a list with scores for specific number of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:12:10.664050Z",
     "start_time": "2019-03-18T20:12:10.579435Z"
    }
   },
   "outputs": [],
   "source": [
    "online_network.load_weights('weights_1350000.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:19:12.935248Z",
     "start_time": "2019-03-18T20:19:12.850159Z"
    }
   },
   "outputs": [],
   "source": [
    "online_network.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:12:11.666797Z",
     "start_time": "2019-03-18T20:12:11.653701Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_dqn(env, n_games, model, nb_actions, skip_start, eps=0.05, render=False, sleep_time=0.01):\n",
    "    scores = []\n",
    "    for i in range(n_games):\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        for skip in range(skip_start):  # skip the start of each game (it's just freezing time before game starts)\n",
    "            obs, reward, done, info = env.step(0)\n",
    "            score += reward\n",
    "        while not done:\n",
    "            state = obs\n",
    "            q_values = model.predict(np.array([state]))[0]\n",
    "            action = epsilon_greedy(q_values, eps, nb_actions)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep_time)\n",
    "                if done:\n",
    "                    time.sleep(1)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:15:16.038811Z",
     "start_time": "2019-03-18T20:13:58.682884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[670.0, 950.0, 2360.0, 320.0, 1120.0]\n"
     ]
    }
   ],
   "source": [
    "ngames = 5\n",
    "scores = test_dqn(env, ngames, online_network, nb_actions, skip_start, render=True)\n",
    "print(scores)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:15:18.486637Z",
     "start_time": "2019-03-18T20:15:18.480313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T20:15:29.670696Z",
     "start_time": "2019-03-18T20:15:29.662123Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are pretty poor since the training was too short. \n",
    "\n",
    "Try to train DQN on a cluster. You might want to adjust some hyperparameters (increase `n_steps`, `warmup`, `copy_steps` and `eps_decay_steps`; gradually decrease learning rate during training, select appropriate `batch_size` to fit gpu memory, adjust `gamma`, switch on double dqn apporach and so on). \n",
    "\n",
    "You can even try to make the network deeper and/or use more than one observation as an input of neural network. For instance, using few consecutive game observations would definetely improve the results since they contain some helpful information such as monsters directions, etc. Turning off TensorBoard callback on a cluster would be a good idea too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
